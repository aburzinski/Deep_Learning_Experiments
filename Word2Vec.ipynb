{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r93-rhaghKMj",
        "colab_type": "text"
      },
      "source": [
        "### Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HZG77tAhCaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-gvIQUrhQI4",
        "colab_type": "text"
      },
      "source": [
        "### Create Required Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxw5VCZehadt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(v):\n",
        "    exps = np.exp(v)\n",
        "    return exps / np.sum(exps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfXfbLAxhoPg",
        "colab_type": "text"
      },
      "source": [
        "### Create Skip-Gram Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPCGheaqhrrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGram:\n",
        "    \n",
        "    def __init__(self, settings):\n",
        "        # n is hidden node size\n",
        "        self.n = settings['n']\n",
        "        \n",
        "        self.eta = settings['learning_rate']\n",
        "        self.epochs = settings['epochs']\n",
        "        self.window_size = settings['window_size']\n",
        "        \n",
        "    def onehot(self, word):\n",
        "        vec = [0 for _ in range(len(self.vocab))]\n",
        "        vec[self.word_index[word]] = 1\n",
        "        return vec\n",
        "        \n",
        "    def generate_training_data(self, settings, corpus):\n",
        "        \n",
        "        word_counts = defaultdict(int)\n",
        "        for row in corpus:\n",
        "            for word in row:\n",
        "                word_counts[word] += 1\n",
        "                \n",
        "        self.vocab = sorted(list(word_counts.keys()), reverse = False)\n",
        "        self.word_index = {word: i for i, word in enumerate(self.vocab)}\n",
        "        self.index_word = {i: word for i, word in enumerate(self.vocab)}\n",
        "        \n",
        "        # V is vocab size\n",
        "        self.V = len(self.vocab)\n",
        "        \n",
        "        training_data = []\n",
        "        for sentence in corpus:\n",
        "            sentence_length = len(sentence)\n",
        "            \n",
        "            for i, word in enumerate(sentence):\n",
        "                \n",
        "                x_t = self.onehot(word)\n",
        "                \n",
        "                x_context = []\n",
        "                for j in range(i - self.window_size, i + self.window_size + 1):\n",
        "                    if j != i and j >= 0 and j < sentence_length:\n",
        "                        x_context.append(self.onehot(sentence[j]))\n",
        "                \n",
        "                training_data.append([x_t, x_context])\n",
        "        return np.array(training_data)\n",
        "        \n",
        "    def forward_pass(self, x):\n",
        "        h = np.dot(np.transpose(self.W1), x)\n",
        "        u_c = np.dot(np.transpose(self.W2), h)\n",
        "        y_c = softmax(u_c)\n",
        "        return y_c, h, u_c\n",
        "    \n",
        "    def back_prop(self, e, h, x):\n",
        "        dl_dW2 = np.outer(h, e)\n",
        "        dl_dW1 = np.outer(x, np.dot(self.W2, e.reshape(-1, 1)))\n",
        "        \n",
        "        self.W2 = self.W2 - self.eta * dl_dW2\n",
        "        self.W1 = self.W1 - self.eta * dl_dW1\n",
        "        pass\n",
        "    \n",
        "    def train(self, training_data):\n",
        "        self.W1 = np.random.uniform(-1, 1, (self.V, self.n))\n",
        "        self.W2 = np.random.uniform(-1, 1, (self.n, self.V))\n",
        "        \n",
        "        for i in range(self.epochs):\n",
        "            self.loss = 0\n",
        "            \n",
        "            for x_t, x_c in training_data:\n",
        "                y_pred, h, u = self.forward_pass(x_t)\n",
        "                \n",
        "                EI = np.sum([np.subtract(y_pred, word) for word in x_c], axis=0)\n",
        "                \n",
        "                self.back_prop(EI, h, x_t)\n",
        "                \n",
        "                self.loss += -np.sum([np.log(np.dot(y_pred, word)) for word in x_c])\n",
        "            \n",
        "            if i % (self.epochs / 10) == 0: print('Epoch: ' + str(i) + ' Loss: ' + str(self.loss))\n",
        "        pass\n",
        "    \n",
        "    def get_word_vec(self, word):\n",
        "        word_idx = self.word_index[word]\n",
        "        return self.W1[word_idx, :]\n",
        "    \n",
        "    def word_sim(self, word, top_n = 5):\n",
        "        word_vec = self.get_word_vec(word)\n",
        "        \n",
        "        word_sim = {}\n",
        "        for vocab_word in self.word_index:\n",
        "            if word != vocab_word:\n",
        "                vec_w2 = self.get_word_vec(vocab_word)\n",
        "                num = np.dot(word_vec, vec_w2)\n",
        "                denom = np.linalg.norm(word_vec) * np.linalg.norm(vec_w2)\n",
        "                word_sim[vocab_word] = num / denom\n",
        "                \n",
        "        words_sorted = sorted(word_sim.items(), key = lambda item: item[1], reverse = True)\n",
        "        \n",
        "        for word, sim in words_sorted[:top_n]:\n",
        "            print(word, sim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5BnNpVn76He",
        "colab_type": "text"
      },
      "source": [
        "### Train Skip-Gram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RZy-QLJkUEn",
        "colab_type": "code",
        "outputId": "4dd9e97f-cdaa-49ec-f1d6-58f2c59320dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "corpus = [['this', 'is', 'the', 'first', 'sentence'],\n",
        "          ['this', 'is', 'the', 'second', 'sentence'],\n",
        "          ['the', 'third', 'sentence', 'is', 'different', 'from', 'the', 'first', 'two'],\n",
        "          ['so', 'is', 'the', 'fourth']]\n",
        "\n",
        "\n",
        "settings = {'n': 10, 'learning_rate': .01, 'epochs': 2000, 'window_size': 2}\n",
        "w2v = SkipGram(settings)\n",
        "\n",
        "training_data = w2v.generate_training_data(settings, corpus)\n",
        "\n",
        "w2v.train(training_data)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: 193.72658241217098\n",
            "Epoch: 200 Loss: 116.96757315913926\n",
            "Epoch: 400 Loss: 116.26246897117\n",
            "Epoch: 600 Loss: 116.10599936148041\n",
            "Epoch: 800 Loss: 116.02523817824081\n",
            "Epoch: 1000 Loss: 115.96739267999725\n",
            "Epoch: 1200 Loss: 115.92040033163948\n",
            "Epoch: 1400 Loss: 115.88025338410868\n",
            "Epoch: 1600 Loss: 115.845181137609\n",
            "Epoch: 1800 Loss: 115.81419021261999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZsUQ5rX8Add",
        "colab_type": "text"
      },
      "source": [
        "### Check Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lLOBNkk4nv3",
        "colab_type": "code",
        "outputId": "2d38645d-d586-4c4c-87fd-1900d5a549c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "w2v.word_sim('second', 5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "third 0.8365993206690362\n",
            "fourth 0.7177220681545701\n",
            "this 0.44937228712435234\n",
            "first 0.44589031511869737\n",
            "different 0.3691023587339741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1nUOtKF8D8l",
        "colab_type": "text"
      },
      "source": [
        "### Create Continuous Bag of Words Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50GBHi7i8GhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBOW:\n",
        "    \n",
        "    def __init__(self, settings):\n",
        "        # n is hidden node size\n",
        "        self.n = settings['n']\n",
        "        \n",
        "        self.eta = settings['learning_rate']\n",
        "        self.epochs = settings['epochs']\n",
        "        self.window_size = settings['window_size']\n",
        "        \n",
        "    def onehot(self, word):\n",
        "        vec = [0 for _ in range(len(self.vocab))]\n",
        "        vec[self.word_index[word]] = 1\n",
        "        return vec\n",
        "        \n",
        "    def generate_training_data(self, settings, corpus):\n",
        "        \n",
        "        word_counts = defaultdict(int)\n",
        "        for row in corpus:\n",
        "            for word in row:\n",
        "                word_counts[word] += 1\n",
        "                \n",
        "        self.vocab = sorted(list(word_counts.keys()), reverse = False)\n",
        "        self.word_index = {word: i for i, word in enumerate(self.vocab)}\n",
        "        self.index_word = {i: word for i, word in enumerate(self.vocab)}\n",
        "        \n",
        "        # V is vocab size\n",
        "        self.V = len(self.vocab)\n",
        "        \n",
        "        training_data = []\n",
        "        for sentence in corpus:\n",
        "            sentence_length = len(sentence)\n",
        "            \n",
        "            for i, word in enumerate(sentence):\n",
        "                \n",
        "                x_t = self.onehot(word)\n",
        "                \n",
        "                x_context = []\n",
        "                for j in range(i - self.window_size, i + self.window_size + 1):\n",
        "                    if j != i and j >= 0 and j < sentence_length:\n",
        "                        x_context.append(self.onehot(sentence[j]))\n",
        "                \n",
        "                training_data.append([x_t, x_context])\n",
        "        return np.array(training_data)\n",
        "        \n",
        "    def forward_pass(self, c_list):\n",
        "        c = np.sum(c_list, axis = 0)\n",
        "        h = np.dot(np.transpose(self.W1), c)\n",
        "        u_w = np.dot(np.transpose(self.W2), h)\n",
        "        y_w = softmax(u_w)\n",
        "        return y_w, h, u_w\n",
        "    \n",
        "    def back_prop(self, e, h, x_list):\n",
        "        x = np.sum(x_list)\n",
        "        dl_dW2 = np.outer(h, e)\n",
        "        dl_dW1 = np.outer(x, np.dot(self.W2, e.reshape(-1, 1)))\n",
        "        \n",
        "        self.W2 = self.W2 - self.eta * dl_dW2\n",
        "        self.W1 = self.W1 - self.eta * dl_dW1\n",
        "        pass\n",
        "    \n",
        "    def train(self, training_data):\n",
        "        self.W1 = np.random.uniform(-1, 1, (self.V, self.n))\n",
        "        self.W2 = np.random.uniform(-1, 1, (self.n, self.V))\n",
        "        \n",
        "        for i in range(self.epochs):\n",
        "            self.loss = 0\n",
        "            \n",
        "            for x_t, x_c in training_data:\n",
        "                y_pred, h, u = self.forward_pass(x_c)\n",
        "                \n",
        "                EI = np.subtract(y_pred, x_t)\n",
        "                \n",
        "                self.back_prop(EI, h, x_c)\n",
        "                \n",
        "                self.loss += -1 * np.log(np.dot(y_pred, x_t))\n",
        "            \n",
        "            if i % (self.epochs / 10) == 0: print('Epoch: ' + str(i) + ' Loss: ' + str(self.loss))\n",
        "        pass\n",
        "    \n",
        "    def get_word_vec(self, word):\n",
        "        word_idx = self.word_index[word]\n",
        "        return self.W1[word_idx, :]\n",
        "    \n",
        "    def word_sim(self, word, top_n = 5):\n",
        "        word_vec = self.get_word_vec(word)\n",
        "        \n",
        "        word_sim = {}\n",
        "        for vocab_word in self.word_index:\n",
        "            if word != vocab_word:\n",
        "                vec_w2 = self.get_word_vec(vocab_word)\n",
        "                num = np.dot(word_vec, vec_w2)\n",
        "                denom = np.linalg.norm(word_vec) * np.linalg.norm(vec_w2)\n",
        "                word_sim[vocab_word] = num / denom\n",
        "                \n",
        "        words_sorted = sorted(word_sim.items(), key = lambda item: item[1], reverse = True)\n",
        "        \n",
        "        for word, sim in words_sorted[:top_n]:\n",
        "            print(word, sim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMqZVF2N9QYk",
        "colab_type": "text"
      },
      "source": [
        "### Train CBOW Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAUqz7oO9TZG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "85e6b4c0-3289-4aee-9bd2-1ed7ffd0f9ba"
      },
      "source": [
        "corpus = [['this', 'is', 'the', 'first', 'sentence'],\n",
        "          ['this', 'is', 'the', 'second', 'sentence'],\n",
        "          ['the', 'third', 'sentence', 'is', 'different', 'from', 'the', 'first', 'two'],\n",
        "          ['so', 'is', 'the', 'fourth']]\n",
        "\n",
        "\n",
        "settings = {'n': 10, 'learning_rate': .01, 'epochs': 20000, 'window_size': 2}\n",
        "w2v = CBOW(settings)\n",
        "\n",
        "training_data = w2v.generate_training_data(settings, corpus)\n",
        "\n",
        "w2v.train(training_data)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: 82.86428218609369\n",
            "Epoch: 2000 Loss: 10.442705091118516\n",
            "Epoch: 4000 Loss: 9.896170470942401\n",
            "Epoch: 6000 Loss: 9.6445903860504\n",
            "Epoch: 8000 Loss: 9.52388844895357\n",
            "Epoch: 10000 Loss: 9.464740190624738\n",
            "Epoch: 12000 Loss: 9.431717901409051\n",
            "Epoch: 14000 Loss: 9.409964885133926\n",
            "Epoch: 16000 Loss: 9.39398624743513\n",
            "Epoch: 18000 Loss: 9.381637976518839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNjykxCJ-nTg",
        "colab_type": "text"
      },
      "source": [
        "### Check Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uixeA6db-pCE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "94a7b1e5-db2f-490c-825d-13f38c7bab8c"
      },
      "source": [
        "w2v.word_sim('second', 5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "third 0.5076415538150308\n",
            "fourth 0.4988226933669053\n",
            "this 0.45611406569040525\n",
            "the 0.2525756156837082\n",
            "first 0.20578864355360912\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}